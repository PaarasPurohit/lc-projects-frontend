{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "section-type: post\n",
    "has-comments: true\n",
    "title: Predicting Customer Reviews with LangChain's Shot-Generative AI\n",
    "category: tech\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Today, I created an application that takes in a customer's review and returns a predicted review from 1 to 5 that accurately represents the customer's review.\n",
    "\n",
    "This project was done using [this](https://www.youtube.com/watch?v=_FpT1cwcSLg&list=PLZoTAELRMXVORE4VF7WQ_fAl0L1Gljtar&index=3) video.\n",
    "\n",
    "The link to the working project will be coming soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Configuration\n",
    "\n",
    "First, I imported the necessary libraries. The only new import here is `FewShotTemplate`, which is generally used if you want to include your prompt, essentially giving you control over the prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import gemini_api_key\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I instantiated the Gemini model for the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = gemini_api_key\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essence of prompt engineering includes wording prompts in better ways and also training AI models to respond better according to user specifications. These specifications come when we give examples of responses to the AI. To do that, I created a JSON object called `examples`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"description\":\"Outstanding service!\",\n",
    "        \"review\":\"5\"\n",
    "    },\n",
    "    {\n",
    "        \"description\":\"Not very good service...\",\n",
    "        \"review\":\"1\"\n",
    "    },\n",
    "    {\n",
    "        \"description\":\"I had an okay time.\",\n",
    "        \"review\":\"3\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I created a simple `PromptTemplate` with two inputs, `description` and `review`, for each column in the JSON examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"description\":\"Outstanding service!\",\n",
    "        \"review\":\"5\"\n",
    "    },\n",
    "    {\n",
    "        \"description\":\"Not very good service...\",\n",
    "        \"review\":\"1\"\n",
    "    },\n",
    "    {\n",
    "        \"description\":\"I had an okay time.\",\n",
    "        \"review\":\"3\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"Description: {description}\n",
    "Review: {review}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I used a different kind of template. Instead of putting the instructions right into the template, I created a template for the output itself to use. You'll see why now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain's FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FewShotPromptTemplate` class is different than a regular `PromptTemplate` because it allows the user to specify more parameters. One of these parameters is `examples`, which allowed me to set my own examples and do prompt engineering.\n",
    "\n",
    "The more examples you add in the JSON, the more accurate it'll be to your specifications.\n",
    "\n",
    "One thing to consider with `FewShotPromptTemplate` is that when passing a `PromptTemplate` for the `example_prompt`, the `template` parameter inside the `PromptTemplate` needs to be for the output itself, not as an instruction. The instruction itself comes in the `FewShotPromptTemplate` parameter `prefix`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix=\"Based on the input, give a review. It should be a number from 1 to 5 and accurately reflect the description input\\n\",\n",
    "    suffix=\"Review: {input}\\nDescription: \",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how I did last time, I'm returning the output as JSON so I can fetch it later. To do that, I first created a chain that uses the instantiated Gemini LLM to respond to `few_shot_prompt`. After that, I made a simple method to return the results in JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = few_shot_prompt\n",
    ")\n",
    "\n",
    "def get_predicted_review(input):\n",
    "    result = chain({'input': input})\n",
    "    \n",
    "    output_json = {\n",
    "        \"description\": result['input'],\n",
    "        \"predicted_review\": result['text'],\n",
    "    }\n",
    "    \n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `main.py`, I created a GET routing to call this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/api/customer', methods=['GET'])\n",
    "def customer_info():\n",
    "    desc = request.args.get('description')\n",
    "    if desc:\n",
    "        response = get_predicted_review(desc)\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Missing 'name' parameter\"}), 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"{{ site.baseurl }}/img/Screenshot (151).png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
