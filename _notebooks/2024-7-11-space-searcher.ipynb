{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "section-type: post\n",
    "has-comments: true\n",
    "title: Getting Started with LangChain - Celestial Object Searcher\n",
    "category: tech\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project was done using [this](https://www.youtube.com/watch?v=_FpT1cwcSLg&list=PLZoTAELRMXVORE4VF7WQ_fAl0L1Gljtar&index=2) video.\n",
    "\n",
    "The link to the working project will be coming soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Gemini\n",
    "\n",
    "Since ChatGPT needed to be paid for, I used Gemini's API. After creating an API key, I started building my environment. \n",
    "\n",
    "I built my project off a Flask template, since I wanted it to be easily deployed through AWS. Here's what `requirements.txt` looked like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Flask\n",
    "requests\n",
    "SQLAlchemy\n",
    "Werkzeug\n",
    "Flask_Login\n",
    "Flask_SqlAlchemy\n",
    "Flask_Migrate\n",
    "Flask_Restful\n",
    "Flask_Cors\n",
    "PyJWT\n",
    "pandas\n",
    "numpy\n",
    "matplotlib\n",
    "seaborn\n",
    "scikit-learn\n",
    "openai\n",
    "langchain\n",
    "streamlit\n",
    "google-generativeai\n",
    "ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `pip install -r requirements.txt`, I was able to start. I followed this [documentation](https://ai.google.dev/gemini-api/docs/get-started/tutorial?lang=python#setup) to do what Krish did in the video without paying for any services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Configuration\n",
    "\n",
    "First, I made `constants.py` and put my Gemini API key there. Then, I imported all the necessary libraries in a file called `demo.py`. This is what my imports looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from constants import gemini_api_key\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, I realized that since this isn't Google Colab, I couldn't use its local libraries, so I did some research. Below is how you can configure your API key to set up in your code (In line 4, I also went ahead and instantiated a model for us to use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = gemini_api_key\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LangChain's PromptTemplate Feature\n",
    "\n",
    "In order to better take input from the user, LangChain has a built-in class called `PromptTemplate`. It allows you to set a variable and a prompt using that variable, then can call your LLM to generate a response. It can then later output those prompt responses as one parent chain.\n",
    "\n",
    "This is useful if you want the user to input only a specific type of query but still get a lot of information.\n",
    "\n",
    "To do that, we need to instantiate the `PromptTemplate` class for every prompt we want, specifying the input variables and template prompts, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input_prompt = PromptTemplate(\n",
    "    input_variables=['name'],\n",
    "    template=\"Tell me about the celestial object by the name of {name}\"\n",
    ")\n",
    "\n",
    "second_input_prompt = PromptTemplate(\n",
    "    input_variables=['name'],\n",
    "    template=\"What are the statistics for the celestial object {name}\"\n",
    ")\n",
    "\n",
    "third_input_prompt = PromptTemplate(\n",
    "    input_variables=['name'],\n",
    "    template=\"Important additional information for celestial object {name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LangChain's ConversationBufferMemory\n",
    "\n",
    "It's good practice when working with AI to store conversations in memory. \n",
    "\n",
    "With our instantiated AI model, we can use LangChain's `ConversationBufferMemory` to store prompts and responses in the conversation memory.\n",
    "\n",
    "To do that, we can instantiate `ConversationBufferMemory` for each prompt we give to the model, storing each conversation memory with its memory key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_memory = ConversationBufferMemory(input_key='name', memory_key='chat_history')\n",
    "stats_memory = ConversationBufferMemory(input_key='name', memory_key='stats_history')\n",
    "additional_info_memory = ConversationBufferMemory(input_key='name', memory_key='info_history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "Now, it's time to actually get responses from our model. To do that, we'll instantiate LLM chains and specify our model as the LLM, our prompts, make sure they're verbose, set the output key, and map the memory keys to the ones we made above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=first_input_prompt,\n",
    "    verbose=True,\n",
    "    output_key='object_info',\n",
    "    memory=object_memory\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=second_input_prompt,\n",
    "    verbose=True,\n",
    "    output_key='stats',\n",
    "    memory=stats_memory\n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=third_input_prompt,\n",
    "    verbose=True,\n",
    "    output_key='additional_info',\n",
    "    memory=additional_info_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll be really painful to keep referencing multiple chains, especially if you want their data and there are more than just three, which is the case most of the time. Fortunately, LangChain has a class called `SequentialChain` that combines multiple chains.\n",
    "\n",
    "To do that, let's instantiate `SequentialChain` to store all our chains in one parent chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_chain = SequentialChain(\n",
    "    chains = [\n",
    "        chain,\n",
    "        chain2,\n",
    "        chain3\n",
    "    ],\n",
    "    input_variables = [\n",
    "        'name'\n",
    "    ],\n",
    "    output_variables = [\n",
    "        'object_info',\n",
    "        'stats',\n",
    "        'additional_info'\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Results\n",
    "\n",
    "You could print the result in the console, but I found it best to store it in JSON. You could keep it as simple as a console app, but JSON allows me to then use the `Flask` library I imported earlier to then put the data on a server to later be fetched by a frontend I create.\n",
    "\n",
    "To do that, let's first create a method that returns the LLM's output in JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celestial_object_info(name):\n",
    "    result = parent_chain({'name': name})\n",
    "    \n",
    "    output_json = {\n",
    "        \"object-information\": result['object_info'],\n",
    "        \"statistics\": result['stats'],\n",
    "        \"additional-information\": result['additional_info'],\n",
    "        \"object-name-buffer\": object_memory.buffer,\n",
    "        \"statistics-buffer\": stats_memory.buffer,\n",
    "        \"additional-information-buffer\": additional_info_memory.buffer\n",
    "    }\n",
    "    \n",
    "    return output_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in `main.py`, route a sub-URL to call this method and return the returned JSON as the response.\n",
    "\n",
    "For the purpose of my app, I made it a GET method, where the request parameter in the URL is the celestial object for query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/api/space', methods=['GET'])\n",
    "def space_info():\n",
    "    name = request.args.get('name')\n",
    "    if name:\n",
    "        response = get_celestial_object_info(name)\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Missing 'name' parameter\"}), 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Here's the console output on the backend:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> Entering new SequentialChain chain...\n",
    "\n",
    "\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "Tell me about the celestial object by the name of Jupiter\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "What are the statistics for the celestial object Jupiter\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "Important additional information for celestial object Jupiter\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "> Finished chain.\n",
    "127.0.0.1 - - [19/Jul/2024 20:08:18] \"GET /api/space?name=Jupiter HTTP/1.1\" 200 -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when I make a `GET` request to `http://127.0.0.1:8086/api/space?name=Jupiter` for example, here's it working:\n",
    "\n",
    "<img src=\"{{ site.baseurl }}/img/Screenshot 2024-07-19 200917.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
